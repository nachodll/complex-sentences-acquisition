{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c873f85e",
   "metadata": {},
   "source": [
    "# Comparative Acquisition of Complex Sentence Structures in Bilingual and Monolingual Children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad4445",
   "metadata": {},
   "source": [
    "## 1. Load all datasets\n",
    "\n",
    "Miami English dataset contains frog story narratives collected in Miami, Florida. Kids 2nd graders (7/8yo) and 5th graders(7/8yo)\n",
    "\n",
    "1. **English-MiamiBiling/**: Bilingual English data\n",
    "2. **English-MiamiMono/**: Monolingual English data\n",
    "3. **Spanish-MiamiBiling/**: Bilingual Spanish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c09ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq as pla\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430ca5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_load_cha(file_path):\n",
    "    \"\"\"\n",
    "    Robust function to load .cha files with fallback strategies\n",
    "    Returns: (success, chat_object_or_utterance_count, method_used)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Strategy 1: Try normal pylangacq loading\n",
    "    try:\n",
    "        chat = pla.read_chat(file_path)\n",
    "        return True, chat, 'pylangacq'\n",
    "    except Exception as e1:\n",
    "        # Strategy 2: Try with explicit encoding\n",
    "        try:\n",
    "            chat = pla.read_chat(file_path, encoding='utf-8')\n",
    "            return True, chat, 'pylangacq_utf8'\n",
    "        except Exception as e2:\n",
    "            # Strategy 3: Raw parsing for basic utterance count\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                lines = content.split('\\n')\n",
    "                chi_utterances = [line for line in lines if line.strip().startswith('*CHI:')]\n",
    "                return False, chi_utterances, 'raw_parsing'\n",
    "            except Exception as e3:\n",
    "                return False, 0, 'failed'\n",
    "\n",
    "def load_directory_structure():\n",
    "    \"\"\"\n",
    "    Comprehensively load all .cha files from all directories\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define all dataset directories\n",
    "    datasets = {\n",
    "        'English-MiamiBiling': {\n",
    "            'bleng2-audio': 'English_Bilingual_Grade2_Audio',\n",
    "            'bleng2-noaudio': 'English_Bilingual_Grade2_NoAudio', \n",
    "            'bleng5-audio': 'English_Bilingual_Grade5_Audio',\n",
    "            'bleng5-noaudio': 'English_Bilingual_Grade5_NoAudio'\n",
    "        },\n",
    "        'English-MiamiMono': {\n",
    "            'mleng2': 'English_Monolingual_Grade2',\n",
    "            'mleng5': 'English_Monolingual_Grade5'\n",
    "        }\n",
    "        # ,\n",
    "        # 'Spanish-MiamiBiling': {\n",
    "        #     'blspan2-audio': 'Spanish_Bilingual_Grade2_Audio',\n",
    "        #     'blspan2-noaudio': 'Spanish_Bilingual_Grade2_NoAudio',\n",
    "        #     'blspan5-audio': 'Spanish_Bilingual_Grade5_Audio', \n",
    "        #     'blspan5-noaudio': 'Spanish_Bilingual_Grade5_NoAudio'\n",
    "        # },\n",
    "    }\n",
    "    \n",
    "    all_datasets = {}\n",
    "    summary_stats = []\n",
    "    \n",
    "    print(\"Loading all datasets...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for main_dir, subdirs in datasets.items():\n",
    "        for subdir, dataset_name in subdirs.items():\n",
    "            dir_path = os.path.join(main_dir, subdir)\n",
    "            \n",
    "            if not os.path.exists(dir_path):\n",
    "                print(f\"âš ï¸  Directory not found: {dir_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Find all .cha files\n",
    "            cha_files = glob.glob(os.path.join(dir_path, \"*.cha\"))\n",
    "            \n",
    "            if not cha_files:\n",
    "                print(f\"ğŸ“ {dataset_name}: No .cha files found\")\n",
    "                continue\n",
    "            \n",
    "            # Load files\n",
    "            dataset_files = []\n",
    "            successful_loads = 0\n",
    "            partial_loads = 0\n",
    "            failed_loads = 0\n",
    "            \n",
    "            for file_path in cha_files:\n",
    "                success, result, method = robust_load_cha(file_path)\n",
    "                \n",
    "                file_info = {\n",
    "                    'filename': os.path.basename(file_path),\n",
    "                    'filepath': file_path,\n",
    "                    'dataset': dataset_name,\n",
    "                    'main_category': main_dir,\n",
    "                    'subcategory': subdir,\n",
    "                    'success': success,\n",
    "                    'method': method\n",
    "                }\n",
    "                \n",
    "                if success:\n",
    "                    file_info['chat_object'] = result\n",
    "                    file_info['participants'] = result.participants()\n",
    "                    file_info['utterance_count'] = len(result.utterances())\n",
    "                    successful_loads += 1\n",
    "                elif method == 'raw_parsing':\n",
    "                    file_info['utterance_count'] = len(result)\n",
    "                    file_info['chi_utterances'] = result\n",
    "                    partial_loads += 1\n",
    "                else:\n",
    "                    failed_loads += 1\n",
    "                \n",
    "                dataset_files.append(file_info)\n",
    "            \n",
    "            # Store dataset\n",
    "            all_datasets[dataset_name] = dataset_files\n",
    "            \n",
    "            # Print summary\n",
    "            total_files = len(cha_files)\n",
    "            print(f\"ğŸ“Š {dataset_name}\")\n",
    "            print(f\"   Total files: {total_files}\")\n",
    "            print(f\"   âœ… Fully loaded: {successful_loads}\")\n",
    "            print(f\"   ğŸ”¶ Partial (raw): {partial_loads}\")\n",
    "            print(f\"   âŒ Failed: {failed_loads}\")\n",
    "            print()\n",
    "            \n",
    "            # Add to summary stats\n",
    "            summary_stats.append({\n",
    "                'dataset': dataset_name,\n",
    "                'main_category': main_dir,\n",
    "                'subcategory': subdir,\n",
    "                'total_files': total_files,\n",
    "                'successful': successful_loads,\n",
    "                'partial': partial_loads,\n",
    "                'failed': failed_loads,\n",
    "                'success_rate': (successful_loads / total_files * 100) if total_files > 0 else 0\n",
    "            })\n",
    "    \n",
    "    return all_datasets, pd.DataFrame(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edba5e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all datasets...\n",
      "============================================================\n",
      "ğŸ“Š English_Bilingual_Grade2_Audio\n",
      "   Total files: 21\n",
      "   âœ… Fully loaded: 12\n",
      "   ğŸ”¶ Partial (raw): 9\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Bilingual_Grade2_NoAudio\n",
      "   Total files: 66\n",
      "   âœ… Fully loaded: 33\n",
      "   ğŸ”¶ Partial (raw): 33\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Bilingual_Grade5_Audio\n",
      "   Total files: 16\n",
      "   âœ… Fully loaded: 7\n",
      "   ğŸ”¶ Partial (raw): 9\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Bilingual_Grade5_NoAudio\n",
      "   Total files: 73\n",
      "   âœ… Fully loaded: 47\n",
      "   ğŸ”¶ Partial (raw): 26\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Monolingual_Grade2\n",
      "   Total files: 43\n",
      "   âœ… Fully loaded: 19\n",
      "   ğŸ”¶ Partial (raw): 24\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Monolingual_Grade5\n",
      "   Total files: 46\n",
      "   âœ… Fully loaded: 29\n",
      "   ğŸ”¶ Partial (raw): 17\n",
      "   âŒ Failed: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data, summary_df = load_directory_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01608c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ COMPREHENSIVE DATASET SUMMARY\n",
      "================================================================================\n",
      "ğŸ“ Total datasets: 6\n",
      "ğŸ“„ Total files: 265\n",
      "âœ… Fully loaded: 147 (55.5%)\n",
      "ğŸ”¶ Partially loaded: 118 (44.5%)\n",
      "âŒ Failed: 0 (0.0%)\n",
      "ğŸ“Š Overall success rate: 100.0%\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ DETAILED BREAKDOWN BY DATASET\n",
      "================================================================================\n",
      "                         dataset       main_category    subcategory  total_files  successful  partial  failed  success_rate\n",
      "  English_Bilingual_Grade2_Audio English-MiamiBiling   bleng2-audio           21          12        9       0     57.142857\n",
      "English_Bilingual_Grade2_NoAudio English-MiamiBiling bleng2-noaudio           66          33       33       0     50.000000\n",
      "  English_Bilingual_Grade5_Audio English-MiamiBiling   bleng5-audio           16           7        9       0     43.750000\n",
      "English_Bilingual_Grade5_NoAudio English-MiamiBiling bleng5-noaudio           73          47       26       0     64.383562\n",
      "      English_Monolingual_Grade2   English-MiamiMono         mleng2           43          19       24       0     44.186047\n",
      "      English_Monolingual_Grade5   English-MiamiMono         mleng5           46          29       17       0     63.043478\n",
      "\n",
      "================================================================================\n",
      "ğŸ” DATASET ACCESS EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š English_Bilingual_Grade2_Audio:\n",
      "   Access: all_data['English_Bilingual_Grade2_Audio']\n",
      "   Files: 21\n",
      "   Example file: 11122020.cha\n",
      "   Utterances (raw): 45\n",
      "\n",
      "ğŸ“š English_Bilingual_Grade2_NoAudio:\n",
      "   Access: all_data['English_Bilingual_Grade2_NoAudio']\n",
      "   Files: 66\n",
      "   Example file: 21122001.cha\n",
      "   Utterances (raw): 20\n",
      "\n",
      "ğŸ“š English_Bilingual_Grade5_Audio:\n",
      "   Access: all_data['English_Bilingual_Grade5_Audio']\n",
      "   Files: 16\n",
      "   Example file: 21132406.cha\n",
      "   Participants: {'CHI'}\n",
      "   Utterances: 24\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ COMPREHENSIVE DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "total_datasets = len(all_data)\n",
    "total_files = sum(len(files) for files in all_data.values())\n",
    "total_successful = summary_df['successful'].sum()\n",
    "total_partial = summary_df['partial'].sum() \n",
    "total_failed = summary_df['failed'].sum()\n",
    "\n",
    "print(f\"ğŸ“ Total datasets: {total_datasets}\")\n",
    "print(f\"ğŸ“„ Total files: {total_files}\")\n",
    "print(f\"âœ… Fully loaded: {total_successful} ({total_successful/total_files*100:.1f}%)\")\n",
    "print(f\"ğŸ”¶ Partially loaded: {total_partial} ({total_partial/total_files*100:.1f}%)\")\n",
    "print(f\"âŒ Failed: {total_failed} ({total_failed/total_files*100:.1f}%)\")\n",
    "print(f\"ğŸ“Š Overall success rate: {(total_successful+total_partial)/total_files*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ DETAILED BREAKDOWN BY DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by main category for better organization\n",
    "summary_df_sorted = summary_df.sort_values(['main_category', 'subcategory'])\n",
    "print(summary_df_sorted.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” DATASET ACCESS EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show examples of how to access data\n",
    "for dataset_name, files in list(all_data.items())[:3]:  # Show first 3 datasets\n",
    "    print(f\"\\nğŸ“š {dataset_name}:\")\n",
    "    print(f\"   Access: all_data['{dataset_name}']\")\n",
    "    print(f\"   Files: {len(files)}\")\n",
    "    \n",
    "    if files:\n",
    "        # Show example file\n",
    "        example_file = files[0]\n",
    "        print(f\"   Example file: {example_file['filename']}\")\n",
    "        if example_file['success']:\n",
    "            print(f\"   Participants: {example_file['participants']}\")\n",
    "            print(f\"   Utterances: {example_file['utterance_count']}\")\n",
    "        else:\n",
    "            print(f\"   Utterances (raw): {example_file.get('utterance_count', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261144c",
   "metadata": {},
   "source": [
    "# 2. Save relevant data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c386c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def build_utterances_dataframe(all_data):\n",
    "    rows = []\n",
    "\n",
    "    for dataset in all_data.keys():\n",
    "        for file in tqdm(all_data[dataset], desc=f\"Processing {dataset}\"):\n",
    "            if file.get('success') is True:\n",
    "                plain_utterances = [\n",
    "                    ' '.join(token.word for token in ut.tokens)\n",
    "                    for ut in file['chat_object'].utterances()\n",
    "                ]\n",
    "\n",
    "                rows.append({\n",
    "                    'dataset': dataset,\n",
    "                    'filename': file['filename'],\n",
    "                    'age': file['chat_object'].ages(),\n",
    "                    'languages': file['chat_object'].languages(),\n",
    "                    'is_bilingual': True if 'Bilingual' in dataset else False,\n",
    "                    'plain_utterances': plain_utterances\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=['dataset', 'filename', 'age', 'languages', 'is_bilingual', 'plain_utterances'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1738453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing English_Bilingual_Grade2_Audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 6475.55it/s]\n",
      "Processing English_Bilingual_Grade2_NoAudio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:00<00:00, 14556.66it/s]\n",
      "Processing English_Bilingual_Grade5_Audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 16276.71it/s]\n",
      "Processing English_Bilingual_Grade5_NoAudio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 5019.91it/s]\n",
      "Processing English_Monolingual_Grade2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:00<00:00, 14606.01it/s]\n",
      "Processing English_Monolingual_Grade5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 13256.70it/s]\n"
     ]
    }
   ],
   "source": [
    "all_utterances_df = build_utterances_dataframe(all_data)\n",
    "all_utterances_df.to_csv('all_utterances.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
