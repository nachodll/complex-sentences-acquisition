{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c873f85e",
   "metadata": {},
   "source": [
    "# Comparative Acquisition of Complex Sentence Structures in Bilingual and Monolingual Children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad4445",
   "metadata": {},
   "source": [
    "## 1. Load all datasets\n",
    "\n",
    "Miami English dataset contains frog story narratives collected in Miami, Florida. Kids 2nd graders (7/8yo) and 5th graders(7/8yo)\n",
    "\n",
    "1. **English-MiamiBiling/**: Bilingual English data\n",
    "2. **English-MiamiMono/**: Monolingual English data\n",
    "3. **Spanish-MiamiBiling/**: Bilingual Spanish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c09ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq as pla\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ca5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_load_cha(file_path):\n",
    "    \"\"\"\n",
    "    Robust function to load .cha files with fallback strategies\n",
    "    Returns: (success, chat_object_or_utterance_count, method_used)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Strategy 1: Try normal pylangacq loading\n",
    "    try:\n",
    "        chat = pla.read_chat(file_path)\n",
    "        return True, chat, 'pylangacq'\n",
    "    except Exception as e1:\n",
    "        # Strategy 2: Try with explicit encoding\n",
    "        try:\n",
    "            chat = pla.read_chat(file_path, encoding='utf-8')\n",
    "            return True, chat, 'pylangacq_utf8'\n",
    "        except Exception as e2:\n",
    "            # Strategy 3: Raw parsing for basic utterance count\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                lines = content.split('\\n')\n",
    "                chi_utterances = [line for line in lines if line.strip().startswith('*CHI:')]\n",
    "                return False, chi_utterances, 'raw_parsing'\n",
    "            except Exception as e3:\n",
    "                return False, 0, 'failed'\n",
    "\n",
    "def load_directory_structure():\n",
    "    \"\"\"\n",
    "    Comprehensively load all .cha files from all directories\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define all dataset directories\n",
    "    datasets = {\n",
    "        'English-MiamiBiling': {\n",
    "            'bleng2-audio': 'English_Bilingual_Grade2_Audio',\n",
    "            'bleng2-noaudio': 'English_Bilingual_Grade2_NoAudio', \n",
    "            'bleng5-audio': 'English_Bilingual_Grade5_Audio',\n",
    "            'bleng5-noaudio': 'English_Bilingual_Grade5_NoAudio'\n",
    "        },\n",
    "        'English-MiamiMono': {\n",
    "            'mleng2': 'English_Monolingual_Grade2',\n",
    "            'mleng5': 'English_Monolingual_Grade5'\n",
    "        }\n",
    "        # ,\n",
    "        # 'Spanish-MiamiBiling': {\n",
    "        #     'blspan2-audio': 'Spanish_Bilingual_Grade2_Audio',\n",
    "        #     'blspan2-noaudio': 'Spanish_Bilingual_Grade2_NoAudio',\n",
    "        #     'blspan5-audio': 'Spanish_Bilingual_Grade5_Audio', \n",
    "        #     'blspan5-noaudio': 'Spanish_Bilingual_Grade5_NoAudio'\n",
    "        # },\n",
    "    }\n",
    "    \n",
    "    all_datasets = {}\n",
    "    summary_stats = []\n",
    "    \n",
    "    print(\"Loading all datasets...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for main_dir, subdirs in datasets.items():\n",
    "        for subdir, dataset_name in subdirs.items():\n",
    "            dir_path = os.path.join(main_dir, subdir)\n",
    "            \n",
    "            if not os.path.exists(dir_path):\n",
    "                print(f\"âš ï¸  Directory not found: {dir_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Find all .cha files\n",
    "            cha_files = glob.glob(os.path.join(dir_path, \"*.cha\"))\n",
    "            \n",
    "            if not cha_files:\n",
    "                print(f\"ðŸ“ {dataset_name}: No .cha files found\")\n",
    "                continue\n",
    "            \n",
    "            # Load files\n",
    "            dataset_files = []\n",
    "            successful_loads = 0\n",
    "            partial_loads = 0\n",
    "            failed_loads = 0\n",
    "            \n",
    "            for file_path in cha_files:\n",
    "                success, result, method = robust_load_cha(file_path)\n",
    "                \n",
    "                file_info = {\n",
    "                    'filename': os.path.basename(file_path),\n",
    "                    'filepath': file_path,\n",
    "                    'dataset': dataset_name,\n",
    "                    'main_category': main_dir,\n",
    "                    'subcategory': subdir,\n",
    "                    'success': success,\n",
    "                    'method': method\n",
    "                }\n",
    "                \n",
    "                if success:\n",
    "                    file_info['chat_object'] = result\n",
    "                    file_info['participants'] = result.participants()\n",
    "                    file_info['utterance_count'] = len(result.utterances())\n",
    "                    successful_loads += 1\n",
    "                elif method == 'raw_parsing':\n",
    "                    file_info['utterance_count'] = len(result)\n",
    "                    file_info['chi_utterances'] = result\n",
    "                    partial_loads += 1\n",
    "                else:\n",
    "                    failed_loads += 1\n",
    "                \n",
    "                dataset_files.append(file_info)\n",
    "            \n",
    "            # Store dataset\n",
    "            all_datasets[dataset_name] = dataset_files\n",
    "            \n",
    "            # Print summary\n",
    "            total_files = len(cha_files)\n",
    "            print(f\"ðŸ“Š {dataset_name}\")\n",
    "            print(f\"   Total files: {total_files}\")\n",
    "            print(f\"   âœ… Fully loaded: {successful_loads}\")\n",
    "            print(f\"   ðŸ”¶ Partial (raw): {partial_loads}\")\n",
    "            print(f\"   âŒ Failed: {failed_loads}\")\n",
    "            print()\n",
    "            \n",
    "            # Add to summary stats\n",
    "            summary_stats.append({\n",
    "                'dataset': dataset_name,\n",
    "                'main_category': main_dir,\n",
    "                'subcategory': subdir,\n",
    "                'total_files': total_files,\n",
    "                'successful': successful_loads,\n",
    "                'partial': partial_loads,\n",
    "                'failed': failed_loads,\n",
    "                'success_rate': (successful_loads / total_files * 100) if total_files > 0 else 0\n",
    "            })\n",
    "    \n",
    "    return all_datasets, pd.DataFrame(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edba5e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all datasets...\n",
      "============================================================\n",
      "ðŸ“Š English_Bilingual_Age2_Audio\n",
      "   Total files: 21\n",
      "   âœ… Fully loaded: 12\n",
      "   ðŸ”¶ Partial (raw): 9\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ðŸ“Š English_Bilingual_Age2_NoAudio\n",
      "   Total files: 66\n",
      "   âœ… Fully loaded: 33\n",
      "   ðŸ”¶ Partial (raw): 33\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ðŸ“Š English_Bilingual_Age5_Audio\n",
      "   Total files: 16\n",
      "   âœ… Fully loaded: 7\n",
      "   ðŸ”¶ Partial (raw): 9\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ðŸ“Š English_Bilingual_Age5_NoAudio\n",
      "   Total files: 73\n",
      "   âœ… Fully loaded: 47\n",
      "   ðŸ”¶ Partial (raw): 26\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ðŸ“Š English_Monolingual_Age2\n",
      "   Total files: 43\n",
      "   âœ… Fully loaded: 19\n",
      "   ðŸ”¶ Partial (raw): 24\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ðŸ“Š English_Monolingual_Age5\n",
      "   Total files: 46\n",
      "   âœ… Fully loaded: 29\n",
      "   ðŸ”¶ Partial (raw): 17\n",
      "   âŒ Failed: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data, summary_df = load_directory_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c01608c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ˆ COMPREHENSIVE DATASET SUMMARY\n",
      "================================================================================\n",
      "ðŸ“ Total datasets: 6\n",
      "ðŸ“„ Total files: 265\n",
      "âœ… Fully loaded: 147 (55.5%)\n",
      "ðŸ”¶ Partially loaded: 118 (44.5%)\n",
      "âŒ Failed: 0 (0.0%)\n",
      "ðŸ“Š Overall success rate: 100.0%\n",
      "\n",
      "================================================================================\n",
      "ðŸ“‹ DETAILED BREAKDOWN BY DATASET\n",
      "================================================================================\n",
      "                       dataset       main_category    subcategory  total_files  successful  partial  failed  success_rate\n",
      "  English_Bilingual_Age2_Audio English-MiamiBiling   bleng2-audio           21          12        9       0     57.142857\n",
      "English_Bilingual_Age2_NoAudio English-MiamiBiling bleng2-noaudio           66          33       33       0     50.000000\n",
      "  English_Bilingual_Age5_Audio English-MiamiBiling   bleng5-audio           16           7        9       0     43.750000\n",
      "English_Bilingual_Age5_NoAudio English-MiamiBiling bleng5-noaudio           73          47       26       0     64.383562\n",
      "      English_Monolingual_Age2   English-MiamiMono         mleng2           43          19       24       0     44.186047\n",
      "      English_Monolingual_Age5   English-MiamiMono         mleng5           46          29       17       0     63.043478\n",
      "\n",
      "================================================================================\n",
      "ðŸ” DATASET ACCESS EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“š English_Bilingual_Age2_Audio:\n",
      "   Access: all_data['English_Bilingual_Age2_Audio']\n",
      "   Files: 21\n",
      "   Example file: 11122020.cha\n",
      "   Utterances (raw): 45\n",
      "\n",
      "ðŸ“š English_Bilingual_Age2_NoAudio:\n",
      "   Access: all_data['English_Bilingual_Age2_NoAudio']\n",
      "   Files: 66\n",
      "   Example file: 21122001.cha\n",
      "   Utterances (raw): 20\n",
      "\n",
      "ðŸ“š English_Bilingual_Age5_Audio:\n",
      "   Access: all_data['English_Bilingual_Age5_Audio']\n",
      "   Files: 16\n",
      "   Example file: 21132406.cha\n",
      "   Participants: {'CHI'}\n",
      "   Utterances: 24\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ˆ COMPREHENSIVE DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "total_datasets = len(all_data)\n",
    "total_files = sum(len(files) for files in all_data.values())\n",
    "total_successful = summary_df['successful'].sum()\n",
    "total_partial = summary_df['partial'].sum() \n",
    "total_failed = summary_df['failed'].sum()\n",
    "\n",
    "print(f\"ðŸ“ Total datasets: {total_datasets}\")\n",
    "print(f\"ðŸ“„ Total files: {total_files}\")\n",
    "print(f\"âœ… Fully loaded: {total_successful} ({total_successful/total_files*100:.1f}%)\")\n",
    "print(f\"ðŸ”¶ Partially loaded: {total_partial} ({total_partial/total_files*100:.1f}%)\")\n",
    "print(f\"âŒ Failed: {total_failed} ({total_failed/total_files*100:.1f}%)\")\n",
    "print(f\"ðŸ“Š Overall success rate: {(total_successful+total_partial)/total_files*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ DETAILED BREAKDOWN BY DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by main category for better organization\n",
    "summary_df_sorted = summary_df.sort_values(['main_category', 'subcategory'])\n",
    "print(summary_df_sorted.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” DATASET ACCESS EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show examples of how to access data\n",
    "for dataset_name, files in list(all_data.items())[:3]:  # Show first 3 datasets\n",
    "    print(f\"\\nðŸ“š {dataset_name}:\")\n",
    "    print(f\"   Access: all_data['{dataset_name}']\")\n",
    "    print(f\"   Files: {len(files)}\")\n",
    "    \n",
    "    if files:\n",
    "        # Show example file\n",
    "        example_file = files[0]\n",
    "        print(f\"   Example file: {example_file['filename']}\")\n",
    "        if example_file['success']:\n",
    "            print(f\"   Participants: {example_file['participants']}\")\n",
    "            print(f\"   Utterances: {example_file['utterance_count']}\")\n",
    "        else:\n",
    "            print(f\"   Utterances (raw): {example_file.get('utterance_count', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca658bee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_kids_stats\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m unique_kids_info = \u001b[43mget_unique_kids_per_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mget_unique_kids_per_dataset\u001b[39m\u001b[34m(all_data)\u001b[39m\n\u001b[32m     13\u001b[39m participants = file_info[\u001b[33m'\u001b[39m\u001b[33mparticipants\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Extract child participants (usually marked as 'CHI' or similar)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m participant_code, participant_info \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparticipants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Look for child participants - typically 'CHI' or participants with child-like codes\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m participant_code.upper() == \u001b[33m'\u001b[39m\u001b[33mCHI\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mCHI\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m participant_code.upper():\n\u001b[32m     19\u001b[39m         \u001b[38;5;66;03m# Use filename as identifier for unique child (since each file represents one child session)\u001b[39;00m\n\u001b[32m     20\u001b[39m         unique_kids.add(file_info[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mAttributeError\u001b[39m: 'set' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "all_data[\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261144c",
   "metadata": {},
   "source": [
    "# 2. Save plain utterances to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3dae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_plain_utterances_from_dataset(dataset):\n",
    "    utterances_dictionary = {}   \n",
    "    for file in tqdm(all_data[dataset]):\n",
    "        plain_utterances = []\n",
    "        if file['success'] == True:\n",
    "            for ut in file['chat_object'].utterances():\n",
    "                plain_utterances.append(' '.join([token.word for token in ut.tokens]))\n",
    "                utterances_dictionary[file['filename']] = [plain_utterances]\n",
    "    return utterances_dictionary\n",
    "\n",
    "def utterances_all_files(datasets, calc_scores=True):\n",
    "    total_utterances_dictionary = {}\n",
    "    for dataset in datasets:\n",
    "        utterances = get_plain_utterances_from_dataset(dataset)\n",
    "        total_utterances_dictionary[dataset] = utterances\n",
    "\n",
    "    return total_utterances_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "534ab50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 7642.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:00<00:00, 9626.65it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 23738.54it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 16222.54it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:00<00:00, 24306.61it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 22873.50it/s]\n"
     ]
    }
   ],
   "source": [
    "all_utterances = utterances_all_files([x for x in summary_df['dataset']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89aa4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "csv_filename = \"all_utterances.csv\"\n",
    "fieldnames = ['dataset', 'filename', 'plain_utterances']\n",
    "\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for dataset, files in all_utterances.items():\n",
    "        # files is a dict: filename -> [plain_utterances, scores] (scores may be absent)\n",
    "        for filename, value in files.items():\n",
    "            plain = value[0] if len(value) > 0 else []\n",
    "            scores = value[1] if len(value) > 1 else None\n",
    "            writer.writerow({\n",
    "                'dataset': dataset,\n",
    "                'filename': filename,\n",
    "                'plain_utterances': json.dumps(plain, ensure_ascii=False)\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
