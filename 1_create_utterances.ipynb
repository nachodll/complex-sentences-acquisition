{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c873f85e",
   "metadata": {},
   "source": [
    "# Comparative Acquisition of Complex Sentence Structures in Bilingual and Monolingual Children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad4445",
   "metadata": {},
   "source": [
    "## 1. Load all datasets\n",
    "\n",
    "The project contains multiple CHILDES datasets organized as follows:\n",
    "\n",
    "1. **English-MiamiBiling/**: Bilingual English data (2 & 5-year-olds, audio/no-audio)\n",
    "2. **English-MiamiMono/**: Monolingual English data (2 & 5-year-olds)  \n",
    "3. **Spanish-MiamiBiling/**: Bilingual Spanish data (2 & 5-year-olds, audio/no-audio)\n",
    "4. **Hoff/**: Additional dataset with English/Spanish bilingual and monolingual data (age groups 2.5, 3, 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c09ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq as pla\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430ca5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_load_cha(file_path):\n",
    "    \"\"\"\n",
    "    Robust function to load .cha files with fallback strategies\n",
    "    Returns: (success, chat_object_or_utterance_count, method_used)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Strategy 1: Try normal pylangacq loading\n",
    "    try:\n",
    "        chat = pla.read_chat(file_path)\n",
    "        return True, chat, 'pylangacq'\n",
    "    except Exception as e1:\n",
    "        # Strategy 2: Try with explicit encoding\n",
    "        try:\n",
    "            chat = pla.read_chat(file_path, encoding='utf-8')\n",
    "            return True, chat, 'pylangacq_utf8'\n",
    "        except Exception as e2:\n",
    "            # Strategy 3: Raw parsing for basic utterance count\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                lines = content.split('\\n')\n",
    "                chi_utterances = [line for line in lines if line.strip().startswith('*CHI:')]\n",
    "                return False, chi_utterances, 'raw_parsing'\n",
    "            except Exception as e3:\n",
    "                return False, 0, 'failed'\n",
    "\n",
    "def load_directory_structure():\n",
    "    \"\"\"\n",
    "    Comprehensively load all .cha files from all directories\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define all dataset directories\n",
    "    datasets = {\n",
    "        'English-MiamiBiling': {\n",
    "            'bleng2-audio': 'English_Bilingual_Age2_Audio',\n",
    "            'bleng2-noaudio': 'English_Bilingual_Age2_NoAudio', \n",
    "            'bleng5-audio': 'English_Bilingual_Age5_Audio',\n",
    "            'bleng5-noaudio': 'English_Bilingual_Age5_NoAudio'\n",
    "        },\n",
    "        'English-MiamiMono': {\n",
    "            'mleng2': 'English_Monolingual_Age2',\n",
    "            'mleng5': 'English_Monolingual_Age5'\n",
    "        },\n",
    "        'Spanish-MiamiBiling': {\n",
    "            'blspan2-audio': 'Spanish_Bilingual_Age2_Audio',\n",
    "            'blspan2-noaudio': 'Spanish_Bilingual_Age2_NoAudio',\n",
    "            'blspan5-audio': 'Spanish_Bilingual_Age5_Audio', \n",
    "            'blspan5-noaudio': 'Spanish_Bilingual_Age5_NoAudio'\n",
    "        },\n",
    "        'Hoff': {\n",
    "            'biling-eng/2.5': 'Hoff_Bilingual_English_Age2.5',\n",
    "            'biling-eng/3': 'Hoff_Bilingual_English_Age3',\n",
    "            'biling-eng/3.5': 'Hoff_Bilingual_English_Age3.5',\n",
    "            'biling-spa/2.5': 'Hoff_Bilingual_Spanish_Age2.5',\n",
    "            'biling-spa/3': 'Hoff_Bilingual_Spanish_Age3',\n",
    "            'biling-spa/3.5': 'Hoff_Bilingual_Spanish_Age3.5',\n",
    "            'mono/2.5': 'Hoff_Monolingual_Age2.5',\n",
    "            'mono/3': 'Hoff_Monolingual_Age3',\n",
    "            'mono/3.5': 'Hoff_Monolingual_Age3.5'\n",
    "        },\n",
    "\n",
    "        'Spanish-Ornat':{\n",
    "            '03': 'Spanish_Ornat_Monolingual_Age3',\n",
    "            '04': 'Spanish_Ornat_Monolingual_Age4',\n",
    "        },\n",
    "        \n",
    "        'Spanish-Sebastian':{\n",
    "            '03': 'Spanish_Sebastian_Monolingual_Age3',\n",
    "            '04': 'Spanish_Sebastian_Monolingual_Age4'\n",
    "   \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_datasets = {}\n",
    "    summary_stats = []\n",
    "    \n",
    "    print(\"Loading all datasets...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for main_dir, subdirs in datasets.items():\n",
    "        for subdir, dataset_name in subdirs.items():\n",
    "            dir_path = os.path.join(main_dir, subdir)\n",
    "            \n",
    "            if not os.path.exists(dir_path):\n",
    "                print(f\"âš ï¸  Directory not found: {dir_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Find all .cha files\n",
    "            cha_files = glob.glob(os.path.join(dir_path, \"*.cha\"))\n",
    "            \n",
    "            if not cha_files:\n",
    "                print(f\"ğŸ“ {dataset_name}: No .cha files found\")\n",
    "                continue\n",
    "            \n",
    "            # Load files\n",
    "            dataset_files = []\n",
    "            successful_loads = 0\n",
    "            partial_loads = 0\n",
    "            failed_loads = 0\n",
    "            \n",
    "            for file_path in cha_files:\n",
    "                success, result, method = robust_load_cha(file_path)\n",
    "                \n",
    "                file_info = {\n",
    "                    'filename': os.path.basename(file_path),\n",
    "                    'filepath': file_path,\n",
    "                    'dataset': dataset_name,\n",
    "                    'main_category': main_dir,\n",
    "                    'subcategory': subdir,\n",
    "                    'success': success,\n",
    "                    'method': method\n",
    "                }\n",
    "                \n",
    "                if success:\n",
    "                    file_info['chat_object'] = result\n",
    "                    file_info['participants'] = result.participants()\n",
    "                    file_info['utterance_count'] = len(result.utterances())\n",
    "                    successful_loads += 1\n",
    "                elif method == 'raw_parsing':\n",
    "                    file_info['utterance_count'] = len(result)\n",
    "                    file_info['chi_utterances'] = result\n",
    "                    partial_loads += 1\n",
    "                else:\n",
    "                    failed_loads += 1\n",
    "                \n",
    "                dataset_files.append(file_info)\n",
    "            \n",
    "            # Store dataset\n",
    "            all_datasets[dataset_name] = dataset_files\n",
    "            \n",
    "            # Print summary\n",
    "            total_files = len(cha_files)\n",
    "            print(f\"ğŸ“Š {dataset_name}\")\n",
    "            print(f\"   Total files: {total_files}\")\n",
    "            print(f\"   âœ… Fully loaded: {successful_loads}\")\n",
    "            print(f\"   ğŸ”¶ Partial (raw): {partial_loads}\")\n",
    "            print(f\"   âŒ Failed: {failed_loads}\")\n",
    "            print()\n",
    "            \n",
    "            # Add to summary stats\n",
    "            summary_stats.append({\n",
    "                'dataset': dataset_name,\n",
    "                'main_category': main_dir,\n",
    "                'subcategory': subdir,\n",
    "                'total_files': total_files,\n",
    "                'successful': successful_loads,\n",
    "                'partial': partial_loads,\n",
    "                'failed': failed_loads,\n",
    "                'success_rate': (successful_loads / total_files * 100) if total_files > 0 else 0\n",
    "            })\n",
    "    \n",
    "    return all_datasets, pd.DataFrame(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba5e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all datasets...\n",
      "============================================================\n",
      "ğŸ“Š English_Bilingual_Age2_Audio\n",
      "   Total files: 21\n",
      "   âœ… Fully loaded: 12\n",
      "   ğŸ”¶ Partial (raw): 9\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Bilingual_Age2_NoAudio\n",
      "   Total files: 66\n",
      "   âœ… Fully loaded: 33\n",
      "   ğŸ”¶ Partial (raw): 33\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Bilingual_Age5_Audio\n",
      "   Total files: 16\n",
      "   âœ… Fully loaded: 7\n",
      "   ğŸ”¶ Partial (raw): 9\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Bilingual_Age5_NoAudio\n",
      "   Total files: 73\n",
      "   âœ… Fully loaded: 47\n",
      "   ğŸ”¶ Partial (raw): 26\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Monolingual_Age2\n",
      "   Total files: 43\n",
      "   âœ… Fully loaded: 19\n",
      "   ğŸ”¶ Partial (raw): 24\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š English_Monolingual_Age5\n",
      "   Total files: 46\n",
      "   âœ… Fully loaded: 29\n",
      "   ğŸ”¶ Partial (raw): 17\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š Spanish_Bilingual_Age2_Audio\n",
      "   Total files: 19\n",
      "   âœ… Fully loaded: 13\n",
      "   ğŸ”¶ Partial (raw): 6\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š Spanish_Bilingual_Age2_NoAudio\n",
      "   Total files: 68\n",
      "   âœ… Fully loaded: 30\n",
      "   ğŸ”¶ Partial (raw): 38\n",
      "   âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š Spanish_Bilingual_Age5_Audio\n",
      "   Total files: 16\n",
      "   âœ… Fully loaded: 11\n",
      "   ğŸ”¶ Partial (raw): 5\n",
      "   âŒ Failed: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data, summary_df = load_directory_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c01608c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ COMPREHENSIVE DATASET SUMMARY\n",
      "================================================================================\n",
      "ğŸ“ Total datasets: 23\n",
      "ğŸ“„ Total files: 853\n",
      "âœ… Fully loaded: 646 (75.7%)\n",
      "ğŸ”¶ Partially loaded: 207 (24.3%)\n",
      "âŒ Failed: 0 (0.0%)\n",
      "ğŸ“Š Overall success rate: 100.0%\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ DETAILED BREAKDOWN BY DATASET\n",
      "================================================================================\n",
      "                           dataset       main_category     subcategory  total_files  successful  partial  failed  success_rate\n",
      "      English_Bilingual_Age2_Audio English-MiamiBiling    bleng2-audio           21          12        9       0     57.142857\n",
      "    English_Bilingual_Age2_NoAudio English-MiamiBiling  bleng2-noaudio           66          33       33       0     50.000000\n",
      "      English_Bilingual_Age5_Audio English-MiamiBiling    bleng5-audio           16           7        9       0     43.750000\n",
      "    English_Bilingual_Age5_NoAudio English-MiamiBiling  bleng5-noaudio           73          47       26       0     64.383562\n",
      "          English_Monolingual_Age2   English-MiamiMono          mleng2           43          19       24       0     44.186047\n",
      "          English_Monolingual_Age5   English-MiamiMono          mleng5           46          29       17       0     63.043478\n",
      "     Hoff_Bilingual_English_Age2.5                Hoff  biling-eng/2.5           70          70        0       0    100.000000\n",
      "       Hoff_Bilingual_English_Age3                Hoff    biling-eng/3           29          29        0       0    100.000000\n",
      "     Hoff_Bilingual_English_Age3.5                Hoff  biling-eng/3.5           40          40        0       0    100.000000\n",
      "     Hoff_Bilingual_Spanish_Age2.5                Hoff  biling-spa/2.5           78          78        0       0    100.000000\n",
      "       Hoff_Bilingual_Spanish_Age3                Hoff    biling-spa/3           39          39        0       0    100.000000\n",
      "     Hoff_Bilingual_Spanish_Age3.5                Hoff  biling-spa/3.5           53          53        0       0    100.000000\n",
      "           Hoff_Monolingual_Age2.5                Hoff        mono/2.5           23          23        0       0    100.000000\n",
      "             Hoff_Monolingual_Age3                Hoff          mono/3           19          19        0       0    100.000000\n",
      "           Hoff_Monolingual_Age3.5                Hoff        mono/3.5           17          17        0       0    100.000000\n",
      "      Spanish_Bilingual_Age2_Audio Spanish-MiamiBiling   blspan2-audio           19          13        6       0     68.421053\n",
      "    Spanish_Bilingual_Age2_NoAudio Spanish-MiamiBiling blspan2-noaudio           68          30       38       0     44.117647\n",
      "      Spanish_Bilingual_Age5_Audio Spanish-MiamiBiling   blspan5-audio           16          11        5       0     68.750000\n",
      "    Spanish_Bilingual_Age5_NoAudio Spanish-MiamiBiling blspan5-noaudio           73          33       40       0     45.205479\n",
      "    Spanish_Ornat_Monolingual_Age3       Spanish-Ornat              03           10          10        0       0    100.000000\n",
      "    Spanish_Ornat_Monolingual_Age4       Spanish-Ornat              04           10          10        0       0    100.000000\n",
      "Spanish_Sebastian_Monolingual_Age3   Spanish-Sebastian              03           12          12        0       0    100.000000\n",
      "Spanish_Sebastian_Monolingual_Age4   Spanish-Sebastian              04           12          12        0       0    100.000000\n",
      "\n",
      "================================================================================\n",
      "ğŸ” DATASET ACCESS EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š English_Bilingual_Age2_Audio:\n",
      "   Access: all_data['English_Bilingual_Age2_Audio']\n",
      "   Files: 21\n",
      "   Example file: 11122020.cha\n",
      "   Utterances (raw): 45\n",
      "\n",
      "ğŸ“š English_Bilingual_Age2_NoAudio:\n",
      "   Access: all_data['English_Bilingual_Age2_NoAudio']\n",
      "   Files: 66\n",
      "   Example file: 11121311.cha\n",
      "   Participants: {'CHI'}\n",
      "   Utterances: 32\n",
      "\n",
      "ğŸ“š English_Bilingual_Age5_Audio:\n",
      "   Access: all_data['English_Bilingual_Age5_Audio']\n",
      "   Files: 16\n",
      "   Example file: 11232046.cha\n",
      "   Utterances (raw): 34\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ COMPREHENSIVE DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "total_datasets = len(all_data)\n",
    "total_files = sum(len(files) for files in all_data.values())\n",
    "total_successful = summary_df['successful'].sum()\n",
    "total_partial = summary_df['partial'].sum() \n",
    "total_failed = summary_df['failed'].sum()\n",
    "\n",
    "print(f\"ğŸ“ Total datasets: {total_datasets}\")\n",
    "print(f\"ğŸ“„ Total files: {total_files}\")\n",
    "print(f\"âœ… Fully loaded: {total_successful} ({total_successful/total_files*100:.1f}%)\")\n",
    "print(f\"ğŸ”¶ Partially loaded: {total_partial} ({total_partial/total_files*100:.1f}%)\")\n",
    "print(f\"âŒ Failed: {total_failed} ({total_failed/total_files*100:.1f}%)\")\n",
    "print(f\"ğŸ“Š Overall success rate: {(total_successful+total_partial)/total_files*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ DETAILED BREAKDOWN BY DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by main category for better organization\n",
    "summary_df_sorted = summary_df.sort_values(['main_category', 'subcategory'])\n",
    "print(summary_df_sorted.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” DATASET ACCESS EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show examples of how to access data\n",
    "for dataset_name, files in list(all_data.items())[:3]:  # Show first 3 datasets\n",
    "    print(f\"\\nğŸ“š {dataset_name}:\")\n",
    "    print(f\"   Access: all_data['{dataset_name}']\")\n",
    "    print(f\"   Files: {len(files)}\")\n",
    "    \n",
    "    if files:\n",
    "        # Show example file\n",
    "        example_file = files[0]\n",
    "        print(f\"   Example file: {example_file['filename']}\")\n",
    "        if example_file['success']:\n",
    "            print(f\"   Participants: {example_file['participants']}\")\n",
    "            print(f\"   Utterances: {example_file['utterance_count']}\")\n",
    "        else:\n",
    "            print(f\"   Utterances (raw): {example_file.get('utterance_count', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9f0ff",
   "metadata": {},
   "source": [
    "# Parse Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To switch languages:\n",
    "language = 'en'  # example: 'en' for English, 'es' for Spanish\n",
    "\n",
    "textstat.set_lang(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3dae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plain_utterances_from_dataset(dataset, calc_scores=True):\n",
    "    utterances_dictionary = {}   \n",
    "    for file in tqdm(all_data[dataset]):\n",
    "        plain_utterances = []\n",
    "        if file['success'] == True:\n",
    "            for ut in file['chat_object'].utterances():\n",
    "                plain_utterances.append(' '.join([token.word for token in ut.tokens]))\n",
    "\n",
    "                if calc_scores:\n",
    "                    scores = [\n",
    "                        file['dataset'],\n",
    "                        file['filename'],\n",
    "                        [textstat.flesch_reading_ease(x) for x in plain_utterances],\n",
    "                        [textstat.fernandez_huerta(x) for x in plain_utterances],\n",
    "                    ]\n",
    "                    \n",
    "                    utterances_dictionary[file['filename']] = [plain_utterances, scores]\n",
    "                else:\n",
    "                    utterances_dictionary[file['filename']] = [plain_utterances]\n",
    "\n",
    "    return utterances_dictionary\n",
    "\n",
    "def utterances_all_files(datasets, calc_scores=True):\n",
    "    total_utterances_dictionary = {}\n",
    "    for dataset in datasets:\n",
    "        utterances = get_plain_utterances_from_dataset(dataset, calc_scores=True)\n",
    "        total_utterances_dictionary[dataset] = utterances\n",
    "\n",
    "    return total_utterances_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ab50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 2625.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:00<00:00, 3474.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 3996.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 2807.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:00<00:00, 3308.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:00<00:00, 3170.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 2373.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 3777.05it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 2667.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 3650.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [04:55<00:00,  4.22s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [02:40<00:00,  5.54s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [03:38<00:00,  5.45s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [06:40<00:00,  5.13s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [03:59<00:00,  6.13s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:47<00:00,  5.43s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [02:05<00:00,  5.47s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [01:56<00:00,  6.16s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:37<00:00,  5.74s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1425.23it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1428.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 922.96it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 1332.86it/s]\n"
     ]
    }
   ],
   "source": [
    "all_utterances = utterances_all_files([x for x in summary_df['dataset']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9d0a09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['English_Bilingual_Age2_Audio', 'English_Bilingual_Age2_NoAudio', 'English_Bilingual_Age5_Audio', 'English_Bilingual_Age5_NoAudio', 'English_Monolingual_Age2', 'English_Monolingual_Age5', 'Spanish_Bilingual_Age2_Audio', 'Spanish_Bilingual_Age2_NoAudio', 'Spanish_Bilingual_Age5_Audio', 'Spanish_Bilingual_Age5_NoAudio', 'Hoff_Bilingual_English_Age2.5', 'Hoff_Bilingual_English_Age3', 'Hoff_Bilingual_English_Age3.5', 'Hoff_Bilingual_Spanish_Age2.5', 'Hoff_Bilingual_Spanish_Age3', 'Hoff_Bilingual_Spanish_Age3.5', 'Hoff_Monolingual_Age2.5', 'Hoff_Monolingual_Age3', 'Hoff_Monolingual_Age3.5', 'Spanish_Ornat_Monolingual_Age3', 'Spanish_Ornat_Monolingual_Age4', 'Spanish_Sebastian_Monolingual_Age3', 'Spanish_Sebastian_Monolingual_Age4'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_utterances.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89aa4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "csv_filename = \"all_utterances.csv\"\n",
    "fieldnames = ['dataset', 'filename', 'plain_utterances', 'scores']\n",
    "\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for dataset, files in all_utterances.items():\n",
    "        # files is a dict: filename -> [plain_utterances, scores] (scores may be absent)\n",
    "        for filename, value in files.items():\n",
    "            plain = value[0] if len(value) > 0 else []\n",
    "            scores = value[1] if len(value) > 1 else None\n",
    "            writer.writerow({\n",
    "                'dataset': dataset,\n",
    "                'filename': filename,\n",
    "                'plain_utterances': json.dumps(plain, ensure_ascii=False),\n",
    "                'scores': json.dumps(scores, ensure_ascii=False)\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
